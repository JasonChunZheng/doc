%!TEX root = paper.tex
%
% AMRClaw
%
% Lead currently:  Marsha Berger
%

\subsection{\amrclaw}
The \amrclaw repository performs block structured adaptive mesh
refinement \cite{Berger:1984ui,Berger:1989gg} for both 
\clawpack and \geoclaw  applications.
A short overview is given
here to set the stage for a description of recent changes.

Currently \amrclaw includes the ability to
\begin{itemize}
\item
coordinate the flagging of points where refinement is needed
either using specified criteria, pre-specified regions, or Richardson extrapolation;
\item
organize the flagged points into efficient refined grid
patches at the next finer level;
\item
initialize newly created fine grids, both solution and
auxiliary arrays (e.g. velocity fields or bathymetry);
\item
orchestrate the time stepping allowing refinement levels to take
different time steps (called sub-cycling in time)
\item
interpolation for ghost cells on fine patches needed before a time step can be taken; and
\item
maintenance of conservation involving correction waves at
the patch boundaries between fine and coarse grids as well as
underneath fine grids.
\end{itemize}

These algorithms have been discussed in detail in
\cite{Berger:1998ia,Berger:2011du}. Roughly one third of the files in \amrclaw
have to be modified for \geoclaw specifics. For example, to maintain a level
sea-level when interpolating the sea-level itself must be interpolated rather than the depth which is the conserved quantity being kept track of.  There are 135 files at this moment 
in the two-dimensional version of \amrclaw.
Forty-five of them are replaced by a \geoclaw-specific file of the
same name but in the \geoclaw 2D branch. Recently three-dimensional version of  \amrclaw has also been brought up to parity with the two-dimensional branch which has seen the most development due to the needs of \geoclaw development.

Parallel efficiency and overall runtime is one aspect that has received more
attention in \clawpack 5. This was partly motivated by the need to run larger
simulations, such as storm surge \cite{Mandli:ws}.  The new emphasis on three-
dimensional AMR applications has also increased interest in this area.  Since
the target machine for use with \clawpack has been single workstations or nodes
of a cluster, the leveraging of multi-core technologies have been the primary
emphasis with parallelizing the software.  Other frameworks including
\forestclaw \cite{Burstedde:we} and \boxlib \todo{citation for pyboxlib?} also
exist and are being developed in parallel with \amrclaw to provide scalable
calculations but on large distributed machines.

Due to the emphasis on multi-core technologies \clawpack has been parallelized
using OpenMP directives based grid-based decomposition.  The main paradigm in
structured AMR is a loop over all grids that exist at a level, where some
operation is done on each grid (e.g. taking a time step, finding ghost cells,
conservation updates, etc.). This lends itself easily to a {\tt parallel for}
loop construct where  each iteration of the loop corresponds to a grid at that
level. Dynamic scheduling is used with a chunk size of one, so that one thread
is assigned one grid at a time.  To help with load balancing, grids at each
level are sorted from largest to smallest workload when they are first created,
using the total number of cells in the grid as an indicator of work. This same
approach is now used in three-dimensions as well.   Note that this approach
causes a memory bulge. Each thread must have its own scratch arrays to save the
incoming and outgoing waves and fluxes for future conservation fix-ups.  The
bulge is directly proportional to the number of threads executing.

%For stack-based memory allocation per thread, the use of the environmental variable {\tt OMP\_STACKSIZE} to increase the limit is necessary.

\missingfigure{Provide some parallel efficiency results.}
\todo{(Kyle) I have data for this and can add a plot if need be.}

One of the other significant additions to \amrclaw algorithmically was the
ability to copy meta-data into newly created grids from their older counterparts
if they overlapped in the correct way.  This can save effort if the computation
of the meta-data is expensive.  In \geoclaw, bathymetry is included in this
meta-data and can be costly to compute for a given cell due to multiple data
sources, mapping on the sphere and interpolation.  Since it is a common
occurrence for a new grid to overlap at least partially older grids, this
ability saves significant computational overhead associated with recomputing
these quantities.

Another new capability just added to both two and three-dimensions is spatially
varying boundary conditions.  For a single grid, it is a simple matter to
compute the location of the ghost cells that extend outside the computational
domain and set them appropriately. With AMR however, the boundary condition
routine can be called for a grid located anywhere in the domain, and may contain
fewer or larger numbers of ghost cells. The boundary condition routines must be
written in a rather unusual way that does not assume it is always setting the
same number of ghost cells, or that the same number of reflected cells inside
the domain always exist. A new computational example of a vortex flowing in from
one side of the boundary is now part of the {\tt Examples} directory to test
this.\todo{Confirm addition of vortex movement, I actually think this may have
been added to the test suite but not the examples.}


